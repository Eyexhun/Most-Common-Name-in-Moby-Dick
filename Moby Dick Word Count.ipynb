{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f88a887",
   "metadata": {},
   "source": [
    "# 1. Tools for text processing\n",
    "\n",
    "We will analyze the most frequent words in Herman Melville's Moby Dick using Python. We'll scrape the novel from Project Gutenberg using the requests library, process the text with BeautifulSoup, and analyze word frequency using nltk and Counter. This pipeline can be adapted to visualize word distributions in any text from Project Gutenberg, showcasing how natural language processing tools apply to unstructured textual data. Let's start by loading the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd100c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requests, BeautifulSoup, nltk, and Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436bd6c3",
   "metadata": {},
   "source": [
    "# 2. Request Moby Dick\n",
    "To analyze Moby Dick, we'll fetch its HTML content from Project Gutenberg, where it is freely available: Moby Dick on Project Gutenberg.\n",
    "\n",
    "HTML (Hypertext Markup Language) is the standard format for web pages, and we can extract its content programmatically. We'll use the requests library to send a GET request, which retrieves the webpage content directly into Python for further processing. This approach mirrors what happens when you visit a webpage in a browser, but here, we automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf3bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\r\n",
      "\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\r\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\r\n",
      "<head>\r\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\r\n",
      "<meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\r\n",
      "<title>The Project Gutenberg eBook of Moby Dick; Or the Whale, by Herman Melville</title>\r\n",
      "\r\n",
      "<style type=\"text/css\" xml:space=\"preserve\">\r\n",
      "\r\n",
      "    body {margin-left:15%; margin-right:15%; text-align:justify }\r\n",
      "    p { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\r\n",
      "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\r\n",
      "    hr  { width: 50%; text-align: center;}\r\n",
      "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\r\n",
      "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\r\n",
      "    .toc       { margin-left: 10%; margin-bottom: .75em;}\r\n",
      "    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\r\n",
      "\r\n",
      "    table      {margin-left: 10%;}\r\n",
      "\r\n",
      "a:link {color:blue;\r\n",
      "\t\ttext-decoration:none}\r\n",
      "link {color:blue;\r\n",
      "\t\ttext-decoration:none}\r\n",
      "a:visited {color:blue;\r\n",
      "\t\ttext-decoration:none}\r\n",
      "a:hover {color:red}\r\n",
      "\r\n",
      "</style>\r\n",
      "  </head>\r\n",
      "  <body>\r\n",
      "\r\n",
      "<div style='text-align:center; font-size:1.2em; font-weight:bold;'>The Project Gutenberg eBook of Moby-Dick; or The Whale, by Herman Melville</div>\r\n",
      "<div style='display:block; margin:1em 0'>\r\n",
      "This eBook is for the use of anyone anywhere in the United States and\r\n",
      "most other parts of the world at no cost and with almost no restrictions\r\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
      "of the Project Gutenberg License included with this eBook or online\r\n",
      "at <a href=\"https://www.gutenberg.org\">www.gutenberg.org</a>. If you\r\n",
      "are not located in the United States, you will have to check the laws of the\r\n",
      "country where you are located before using this eBoo\n"
     ]
    }
   ],
   "source": [
    "# Getting the Moby Dick HTML \n",
    "r = requests.get(\"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\")\n",
    "\n",
    "# Setting the correct text encoding of the HTML page\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "# Extracting the HTML from the request object\n",
    "html = r.text\n",
    "\n",
    "# Printing the first 2000 characters in html\n",
    "print(html[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bda27",
   "metadata": {},
   "source": [
    "# 3. Get the text from the HTML\n",
    "The HTML from Project Gutenberg contains the text of Moby Dick, but it needs cleaning and extraction before we can use it. We'll use the BeautifulSoup library for this task.\n",
    "\n",
    "The name \"Beautiful Soup\" comes from its ability to clean and parse \"tag soup\"—HTML that might be messy or non-standard. The library makes it easy to extract the specific content we need. We'll create a BeautifulSoup object to process the HTML and isolate the actual text of the novel, leaving behind unnecessary web elements like navigation links and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74865732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inging up the rear\r\n",
      "      of every funeral I meet; and especially whenever my hypos get such an\r\n",
      "      upper hand of me, that it requires a strong moral principle to prevent me\r\n",
      "      from deliberately stepping into the street, and methodically knocking\r\n",
      "      people’s hats off—then, I account it high time to get to sea as soon\r\n",
      "      as I can. This is my substitute for pistol and ball. With a philosophical\r\n",
      "      flourish Cato throws himself upon his sword; I quietly take to the ship.\r\n",
      "      There is nothing surprising in this. If they but knew it, almost all men\r\n",
      "      in their degree, some time or other, cherish very nearly the same feelings\r\n",
      "      towards the ocean with me.\r\n",
      "    \n",
      "\r\n",
      "      There now is your insular city of the Manhattoes, belted round by wharves\r\n",
      "      as Indian isles by coral reefs—commerce surrounds it with her surf.\r\n",
      "      Right and left, the streets take you waterward. Its extreme downtown is\r\n",
      "      the battery, where that noble mole is washed by waves, and cooled by\r\n",
      "      breezes, which a few hours previous were out of sight of land. Look at the\r\n",
      "      crowds of water-gazers there.\r\n",
      "    \n",
      "\r\n",
      "      Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\r\n",
      "      Hook to Coenties Slip, and from thence, by Whitehall, northward. What do\r\n",
      "      you see?—Posted like silent sentinels all around the town, stand\r\n",
      "      thousands upon thousands of mortal men fixed in ocean reveries. Some\r\n",
      "      leaning against the spiles; some seated upon the pier-heads; some looking\r\n",
      "      over the bulwarks of ships from China; some high aloft in the rigging, as\r\n",
      "      if striving to get a still better seaward peep. But these are all\r\n",
      "      landsmen; of week days pent up in lath and plaster—tied to counters,\r\n",
      "      nailed to benches, clinched to desks. How then is this? Are the green\r\n",
      "      fields gone? What do they here?\r\n",
      "    \n",
      "\r\n",
      "      But look! here come more crowds, pacing straight for the water, and\r\n",
      "      seemingly bound for a dive. Strange! Nothing w\n"
     ]
    }
   ],
   "source": [
    "# Creating a BeautifulSoup object from the HTML\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "# Getting the text out of the soup\n",
    "text = soup.get_text()\n",
    "\n",
    "# Printing out text between characters 32000 and 34000\n",
    "\n",
    "print(text[32000:34000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41310054",
   "metadata": {},
   "source": [
    "# 4. Extract the words\n",
    "Now that we have the text of Moby Dick, we can move on to analyzing the word frequencies. Although there’s some extraneous content at the beginning and end, it’s negligible compared to the bulk of the novel and can be ignored for now.\n",
    "\n",
    "To count word occurrences, we’ll use the Natural Language Toolkit (nltk). The first step is tokenization: breaking the text into individual words by removing non-word elements like whitespace and punctuation. This process results in a clean list of words, ready for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7199d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Moby', 'Dick', 'Or']\n"
     ]
    }
   ],
   "source": [
    "# Creating a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "print(tokens[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65ae01",
   "metadata": {},
   "source": [
    "# 5. Make the words lowercase\n",
    "To ensure accurate word counts, we need to treat words like \"Or\" and \"or\" as the same. The solution is to convert all words to lowercase before counting their occurrences. This normalization step avoids case sensitivity issues and ensures consistent word representation.\n",
    "\n",
    "We'll modify our list of words from Moby Dick by applying the .lower() method to each word. This will create a uniform, lowercase representation of all words in the text, preparing them for accurate frequency analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6ae12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or']\n"
     ]
    }
   ],
   "source": [
    "# Create a list called words containing all tokens transformed to lower-case\n",
    "words = [tokens.lower() for tokens in tokens]\n",
    "# Printing out the first 8 words / tokens \n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c4200",
   "metadata": {},
   "source": [
    "# 6. Load in stop words\n",
    "To focus on meaningful and interesting words, we commonly remove high-frequency but less informative words, known as stop words (e.g., \"the,\" \"of,\" \"a\"). The nltk library provides a built-in list of English stop words that we can use for this purpose.\n",
    "\n",
    "We’ll filter out these stop words from the tokenized and normalized word list. This step helps highlight the unique vocabulary and themes of Moby Dick by focusing on the content-rich words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e144bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91984\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure the stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Getting the English stop words\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "print(sw[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fefddc5",
   "metadata": {},
   "source": [
    "# 7. Remove stop words in Moby Dick\n",
    "To exclude stop words from Moby Dick, use list comprehension to filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb4c0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'moby', 'dick']\n"
     ]
    }
   ],
   "source": [
    "# Create a list words_ns containing all words that are in words but not in sw\n",
    "words_ns = [words for words in words if words not in sw]\n",
    "# Printing the first 5 words_ns to check that stop words are gone\n",
    "print(words_ns[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a920e",
   "metadata": {},
   "source": [
    "# 8. We have the answer\n",
    "To identify the most frequent words in Moby Dick, we’ll use the Counter class from Python's collections module. Here's the process:\n",
    "\n",
    "1. Pass the filtered_words list to Counter to create a dictionary-like object where keys are words and values are their counts.\n",
    "2. Use the .most_common(n) method to retrieve the top n most frequent words along with their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8439a9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whale', 1244), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Counter object from our processed list of words\n",
    "count = Counter(words_ns)\n",
    "# Store 10 most common words and their counts as top_ten\n",
    "top_ten = count.most_common(10)\n",
    "# Print the top ten words and their counts\n",
    "print(top_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345aa46",
   "metadata": {},
   "source": [
    "# 9. The most common word\n",
    "The most common word in Moby Dick is likely \"whale\", reflecting the novel's main theme. This analysis shows how natural language processing can extract insights from unstructured text, a key skill for working with various types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71969569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whale\n"
     ]
    }
   ],
   "source": [
    "# What's the most common word in Moby Dick?\n",
    "most_common_word = count.most_common(1)[0][0]\n",
    "print(most_common_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
